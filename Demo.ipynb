{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What is a neural network?\n",
    "Neural network is a machine learning method that is loosely inspired by neurons in the brain. What they do is multiply weights and inputs. If we're given a training set of inputs and outputs it will learn the mapping by continously updating weights to get nearer to the output. \n",
    "### Neural Networks look like this\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Neural_Network(input, weight):\n",
    "    return input * weight\n",
    "weight = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04018835])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neural_Network(0.2354, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what neural networks do! They multiply weights and inputs to get a prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working  with multiple inputs and weights\n",
    "For this example we'll work with 4 inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.76592401  3.87182717  2.21030535  0.10135316]\n"
     ]
    }
   ],
   "source": [
    "weight = np.random.randn(4)\n",
    "np.random.seed(1)\n",
    "input = np.array([2.3, 4.6, 7.8, 0.9])\n",
    "print(Neural_Network(weight, input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We get more than one result. So, we need to sum these all results to get a prediction. So, We need to define a weighted sum funtion. We can do this by numpy 'dot' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41756167300040448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(input, weight)#We're gonna be using this instead of Neural_Network function that we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what the neural network that we coded looks like\n",
    "![This is how our neural network looks like](http://www.theprojectspot.com/images/post-assets/an.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do this with an example\n",
    "#### Will it rain or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is this Activation Function(f) in the above picture\n",
    "What if we want to predict whether it'll rain or not. We can have some inputs(features)) 'X', Weights ' W' and Output 'Y'\n",
    "The formula would be \" X * W = Y' where 'Y' is the probability of Raining. From the program above that's not the case as it is not the prbability. That's were activation functions come in play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of activation functions. Here we're gonna be using Sigmoid function that will convert the result into a probability between 0 and 1\n",
    "![Activatio Function](https://qph.ec.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602899632657\n"
     ]
    }
   ],
   "source": [
    "#We need to define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "#Now take the sigmoid of the dot product\n",
    "print(sigmoid(np.dot(input, weight)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model predicts that there is 99% chance of raining. But this may be wrong as multiplying inputs and random weights will give us random results. We need the correct weghts for the model to predict accurately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do this with an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "inputs = np.array([[1, 0, 1], [0, 0, 1], [1, 1, 1], [0,0,0]]) #inputs\n",
    "outputs = np.array([[1],[0],[1],[0]]) #Expected outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we can't change our inputs and our output is dependent on the weights we give it.\n",
    "#### Random weights = Random outputs \n",
    "So, we need a way to optimize our weights such that, it will give us the results we want.\n",
    "But, First we initialize the weights randomly and then, optimize it over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function \n",
    "def sigmoid(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)#derivative\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60000):#here, 60000 is the no. of iterations\n",
    "    pred = sigmoid(np.dot(inputs, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Now we need to optimize weights. The way we're gonna do that is by using gradient descent(one of the trickiest topics to learn). Let's visualize this! Then I'll explain it. Checkout [this](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/) tutorial. It's great\n",
    "\n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-b7a3a254830ac374818cdce3fa5a7f17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go against the gradient to get to the local minima(Not math) \n",
    "    Through calculus, we derive the equations for gradient descents\n",
    "^^^ image where there's a graph plotted for 1 weight and the error. For, every iteration we calculate the gradient(slope) to get to the local minima(The point where the error is  lowest). And Descenting down the gradient will give us a sene of direction(pos or neg). Then we update the weights by going against the gradient. Check out [this video](https://www.youtube.com/watch?v=jc2IthslyzM) for the math and for the foundation for the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60000):\n",
    "\tpred = sigmoid(np.dot(inputs, weights))\n",
    "\terror = pred- outputs\n",
    "\tadjustments = np.dot(inputs.T, error* sigmoid(pred, deriv=True))\n",
    "\tweights -= adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your neural network has finished training!. Let us test this...\n",
      "Considering New situation ->> [1, 1, 0]\n",
      "predicting..\n",
      "[ 0.99999681]\n"
     ]
    }
   ],
   "source": [
    "#Time to test it\n",
    "print('Your neural network has finished training!. Let us test this...')\n",
    "print('Considering New situation ->> [1, 1, 0]')\n",
    "output_for_test = sigmoid(np.dot(np.array([1,1,0]), weights))\n",
    "print('predicting..')\n",
    "print(output_for_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ See, 0.9999 was't that bad\n",
    "##### Congratutlations. You just made a neural network from scratch! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "#To increase the accuracy you can add more hidden layers, more examples.If you are working on a complex problem\n",
    "#Example of the same neural network bu with a hidden layer\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print (\"Output After Training:\")\n",
    "print (l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
